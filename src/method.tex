\par{DL\_POLY is extensively parallelised and optimised for distributed systems using MPI 2.0\cite{mpi1994}.}

\par{Being a general purpose tool one can simulate a large variety of systems. However for simplicity we decided to concentrate
on systems of interest for two communities: biophysics and material science.}

\par{The system of interest for biophysics community chosen is case 7 in DL\_POLY testing suite, we shall refer to it from now on 
as Gramidicin and it consists of 8 Gramidicin A solvated in water. The system has a tolal of 99120 atoms. The simulation is 
carried out in an NPT ensemble at room temperature. An equivalent larger system with 792960 is provided, case 8.}

\par{The system of interest for material science community is case 31 in DL\_POLY testing suite, we shall refer to it fom now on 
as Iron and it consists of an Iron BCC with a constant lattice of 2.8665~\AA~with 31250 atoms. The simulation as previously
is carried out in an NPT ensemble at room temperature. A larger system with 250000 atoms is provided by test case 32.}

\par{With the exception of the large Gramidicin system all the systems comfortably fit in the 8~GiB memory of the 
Xeon Phi we have available.}

\par{All the simulations reported here were carried out on ICHEC's system fionn\cite{fionn}. The Xeon Phi 
partition consists of 16 nodes each Host with two sockets Ivy Bridge Xeon, E2660-v2\cite{xeon}, and 
two Xeon Phi cards, 5110P\cite{xeonphi5110P}. The memory available on the host is 64~GiB and for each card 8~GiB.}

\par{One indicator of the code performance traditionally used by molecular dynamics communities is average time per molecular 
dynamics integration step. We will call it \emph{time per step}. This metric will be employed by us to assess the performance of 
DL\_POLY, this can be easily transformed into other customary metrics, \emph{eg.} simulation time per 24 hours of walltime, as 
employed by GROMACS\cite{Berendsen1995} and NAMD\cite{Kale1999}.}

\par{Preliminary scalability curves for the two systems of interest were carried out, see \fref{fig:base}. We used the latest 
stable version of DL\_POLY 4.05.01, with Intel Fortran compiler, version 14.0.1, and Intel MPI implementation, version 4.1.2.040, 
current stable versions at time of computing. Very good scalling is shown for both systems, Gramidicin in upper right panel, 68\% 
relative efficiency, and Iron in the lower right panel, 73\% relative efficiency, in the case of the Host for 20 MPI processes,
for the time step. However, when run on the Xeon Phi co-processor in native mode the relative efficiency for both 
systems is at around 50\% up to 60 processes and then suddenly degrades when more MPI processes are added. This poor performance 
was tracked down to excessive time taken by MPI communication in the Xeon Phi. The ratio MPI communication time per calculation 
time is as high as 75\% when running with 120 MPI processes, making pure native mode not an attractive running mode without 
major design changes of the algorithms used.}

\par{The three avenues left to investigate are OpenMP native, offload and MPI symmetric modes. In the results sections of this 
communication we will present preliminary results of our investigations.}
%
\begin{figure}[!ht]
\centering
\begin{tikzpicture}[]
\centering\tikzstyle{every node}=[font=\tiny]
\begin{groupplot}[
    group style={
        group name=my plots,
        group size=2 by 2,
        xlabels at=edge bottom,
    %    xticklabels at=edge bottom,
        horizontal sep=1mm, vertical sep=0.5cm,
    },
    /tikz/mark size=1.0pt,
    width=0.45\textwidth,
    minor tick num =4,ymin=0,ymax=1.0,
    grid=none,
    legend pos=outer north east,
    legend cell align=left,
    legend style={xshift=5mm},
    every axis label/.append style={font=\footnotesize},
    x tick label style={scaled x ticks = false,
        /pgf/number format/fixed,},
    y tick label style={scaled y ticks = false,
        /pgf/number format/fixed,},
]
\nextgroupplot[ylabel={Efficiency}, xmin=1,xmax=20,
 xmode=log, log basis x=2,
 xtick={1,2,4,8,10,16,20},xticklabels={1,2,4,8,10,16,20}]
]
\addplot +[blue,thick,solid] table[x index=0, y index=  2]  {figures/gramBaseHostMPI.dat};
\addplot +[red,thick,solid] table[x index=0, y index=  4]  {figures/gramBaseHostMPI.dat};
\addplot +[green,thick,solid] table[x index=0, y index=  6]  {figures/gramBaseHostMPI.dat};
\addplot +[pink,thick,solid] table [x index=0, y index=  8] {figures/gramBaseHostMPI.dat};
\nextgroupplot[ylabel={},yticklabel pos=right, ylabel near ticks,xmin=1,xmax=144,
 xmode=log, log basis x=2,
 xtick={1,2,4,8,10,16,20,32,60,120,144},xticklabels={1,2,4,8,10,16,20,32,60,,144}]
]
\addplot +[blue,thick,solid] table [x index=0, y index=  2] {figures/gramBaseMICMPI.dat};
\addplot +[red,thick,solid] table [x index=0, y index=  4]{figures/gramBaseMICMPI.dat};
\addplot +[green,thick,solid] table[x index=0, y index=  6] {figures/gramBaseMICMPI.dat};
\addplot +[pink,thick,solid] table[x index=0, y index=  8] {figures/gramBaseMICMPI.dat};
\legend{ time per step,
two body forces,
shake,
linked lists}
\nextgroupplot[ylabel={Efficiency}, xlabel={MPI Processes},xmin=1,xmax=20,
 xmode=log, log basis x=2,
 xtick={1,2,4,8,10,16,20},xticklabels={1,2,4,8,10,16,20}]
]
\addplot +[blue,thick,solid] table[x index=0, y index=  2]  {figures/ironBaseHostMPI.dat};
\addplot +[green,thick,solid] table[x index=0, y index=  4]  {figures/ironBaseHostMPI.dat};
\addplot +[red,thick,solid] table[x index=0, y index=  6]  {figures/ironBaseHostMPI.dat};
\addplot +[pink,thick,solid] table [x index=0, y index=  8] {figures/ironBaseHostMPI.dat};
\nextgroupplot[ylabel={},xlabel={MPI Processes},yticklabel pos=right, ylabel near ticks,xmin=1,xmax=240,
 xmode=log, log basis x=2,
 xtick={1,2,4,8,10,16,20,32,60,120,144,196,240},xticklabels={1,2,4,8,10,16,20,32,60,120,,,240}]
]
\addplot +[blue,thick,solid] table [x index=0, y index=  2] {figures/ironBaseMICMPI.dat};
\addplot +[green,thick,solid] table [x index=0, y index=  4]{figures/ironBaseMICMPI.dat};
\addplot +[red,thick,solid] table[x index=0, y index=  6] {figures/ironBaseMICMPI.dat};
\addplot +[pink,thick,solid] table[x index=0, y index=  8] {figures/ironBaseMICMPI.dat};
\legend{time per step,
metal local density,
two body forces,
linked lists}
\end{groupplot}
\node[] at (14.0,3.0) {Gramidicin};
\node[] at (14.0,-2.5) {Iron};
\end{tikzpicture}
\caption{MPI Scallability for two systems Gramidicin (upper panels) and Iron(lower panels). 
	Host scalability is reported in the left hand side and Xeon Phi in the right hand side. Full data is reported 
  in Tables \jref{tab:gramHMPI}, \jref{tab:gramMICMPI}, \jref{tab:ironHMPI} and \jref{tab:ironMICMPI}}
\label{fig:base}
\end{figure}


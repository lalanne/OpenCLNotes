\par{Based on our previous results in Native Mode it is obvious that our OpenMP implementation for MD does not make efficient use
of all the threads available on the Xeon Phi, one way to tackle this issue is to use the co-processor in offload mode. From the 
code segments identified earlier the best candidates for offloading is two body forces due to its good OpenMP scaling on Xeon Phi. 
Shake is the only code segment which due to the MPI synchronization at each iteration is not suitable for offloading, 
Linked Lists and metal local density code segments due to poor OpenMP scalability on the Xeon Phi can be challenging, 
for the rest of this section we will concentrate on the offload of two body forces.}

\par{One can offload data to the co-processor in two ways, using OpenMP 4.0 or Intel LEO (Language Extentions for Offload). As the
code already uses OpenMP for parallelisation is a natural option to use OpenMP also for offloading, unfortunatelly after 
implementing the offload code with OpenMP we discovered an issue with implementation of OpenMP tasks and offload. In order to 
avoid this we changed the offload implementation to Intel LEO. Both, OpenMP and LEO, implementations for the synchronous 
offloading achieved the same performance results.}

\par{One has to upload for each computation to the card: positions, the list of neighbours for each atom and all data related to 
the potential and download from the card to the host the contribution to forces and other statistical quantities.}

\par{In the Gramidicin case when running 1 MPI the data transfered to the Xeon Phi can be as big as 333.54MB and the data 
transfered from the Xeon Phi to the Host \footnote{Intel(R) Xeon(R) CPU E5-2660 v2, 2.20GHz, 64GiB.} is 11.08MB for every time 
step of the simulation, the time spent doing this is 0.24 seconds. By far the biggest chunk of data is associated with 
linked lists.}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\centering\tikzstyle{every node}=[font=\tiny]
\begin{groupplot}[
    group style={
        group name=my plots,
        group size=2 by 2,
        xlabels at=edge bottom,
    %    xticklabels at=edge bottom,
        horizontal sep=3mm, vertical sep=0.5cm,
    },
    /tikz/mark size=1.0pt,
    width=0.45\textwidth,
    minor tick num =4,ymin=0,ymax=1.0,
    grid=none,
    legend pos=outer north east,
    legend cell align=left,
    legend style={xshift=5mm},
    every axis label/.append style={font=\footnotesize},
    x tick label style={scaled x ticks = false,
        /pgf/number format/fixed,},
    y tick label style={scaled y ticks = false,
        /pgf/number format/fixed,},
]
\nextgroupplot[ylabel={Time [s]}, xmin=1,xmax=20,ymin=0,ymax=1.4,
 xmode=log, log basis x=2,
 xtick={1,2,4,8,10,16,20},xticklabels={1,2,4,8,10,16,20},ytick={0.0,0.2,0.4,0.6,0.8,1.0,1.2,1.4}]
\addplot +[blue,thick,solid] table[x index=0, y index=  1]  {figures/gram-off-leo-1card-5X.dat};
\addplot +[red,thick,solid] table[x index=0, y index=  2]  {figures/gram-off-leo-1card-5X.dat};
\addplot +[green,thick,solid] table[x index=0, y index=  3]  {figures/gram-off-leo-1card-5X.dat};
\addplot +[pink,thick,solid] table [x index=0, y index=  4] {figures/gram-off-leo-1card-5X.dat};
\nextgroupplot[ylabel={},yticklabel pos=right, ylabel near ticks,xmin=2,xmax=20,
 xmode=log, log basis x=2,
 xtick={2,4,8,10,16,20},xticklabels={2,4,8,10,16,20}]
\addplot +[blue,thick,solid] table [x index=0, y index=  1] {figures/gram-off-leo-2cards-5X.dat};
\addplot +[red,thick,solid] table [x index=0, y index=  2]{figures/gram-off-leo-2cards-5X.dat};
\addplot +[green,thick,solid] table[x index=0, y index=  3] {figures/gram-off-leo-2cards-5X.dat};
\addplot +[pink,thick,solid] table[x index=0, y index=  4] {figures/gram-off-leo-2cards-5X.dat};
\legend{ time per step,
two body forces,
shake,
linked lists}
\nextgroupplot[ylabel={Time [s]}, xlabel={MPI Processes},xmin=1,xmax=24,ymin=0,ymax=1.4,
 xmode=log, log basis x=2,
 xtick={1,2,4,6,8,10,12,16,20,24},xticklabels={1,2,4,6,8,10,12,16,20,24},ytick={0.0,0.2,0.4,0.6,0.8,1.0,1.2,1.4}]
\addplot +[blue,thick,solid] table[x index=0, y index=  1]  {figures/gram-off-leo-1card-7X.dat};
\addplot +[red,thick,solid] table[x index=0, y index=  2]  {figures/gram-off-leo-1card-7X.dat};
\addplot +[green,thick,solid] table[x index=0, y index=  3]  {figures/gram-off-leo-1card-7X.dat};
\addplot +[pink,thick,solid] table [x index=0, y index=  4] {figures/gram-off-leo-1card-7X.dat};
\nextgroupplot[ylabel={},xlabel={MPI Processes},yticklabel pos=right, ylabel near ticks,xmin=2,xmax=24,
 xmode=log, log basis x=2,
 xtick={2,4,6,8,10,12,16,20,24},xticklabels={2,4,6,8,10,12,16,20,24}]
\addplot +[blue,thick,solid] table [x index=0, y index=  1] {figures/gram-off-leo-2cards-7X.dat};
\addplot +[red,thick,solid] table [x index=0, y index=  2]{figures/gram-off-leo-2cards-7X.dat};
\addplot +[green,thick,solid] table[x index=0, y index=  3] {figures/gram-off-leo-2cards-7X.dat};
\addplot +[pink,thick,solid] table[x index=0, y index=  4] {figures/gram-off-leo-2cards-5X.dat};
\end{groupplot}
\node[] at (14.0,3.0) {Xeon Phi 5110P};
\node[] at (14.0,-2.5) {Xeon Phi 7120A};
\end{tikzpicture}
\caption{Best results for DL\_POLY with synchronous offloading for two body forces in the case of Gramidicin, 
upper panels show data for the 5110P model card lower pannels for the 7120A model card vs MPI process count. 
For each case both host and Xeon Phi are saturated with OpenMP threads.
\jref{tab:gramOffLeoMPI15X}, \jref{tab:gramOffLeoMPI25X}, \jref{tab:gramOffLeoMPI17X} 
  and \jref{tab:gramOffLeoMPI27X}}
\label{fig:offleo}
\end{figure}

\par{We carried a systematic study of the offload performance when multiple MPI processes offload the code to one or two cards 
for two body forces the results are presented in \fref{fig:offleo}. Upper panels of \fref{fig:offleo} present results for 
offloading to the 5110P model \footnote{8gb, stepping B1, 1052.630 ghz, 60 cores.} of the Intel Xeon Phi and lower panels show 
the results for the model 7120A \footnote{ 16gb,  C0 stepping, 1.238 ghz, 61 cores, C0QS-7120 (pre-prod 7120A) ivb-ep 12c 
2.5ghz, 128gb.} of the Intel Xeon Phi. The results presented in \fref{fig:offleo} were obtained by running DL\_POLY with a
varying number of MPI processes, when two cards are used for offloading first half of MPI processes offload to card number 0
and the second half to card number 1. For each case when the number of MPI processes was less than the number of cores of the
Host where possible we saturated each MPI process with OpenMP threads. For each MPI process the card was equally partitioned 
and saturated with OpenMP threads. The best results for each MPI process count is selected and presented.}

\par{When we offload to one card, left hand side panel, or two cards, right hand side panel, using more than 4 MPI processes 
offloading to the same card slows down two body forces. This is due to the fact that data transfers dominate over computations, 
hence the need of using a reduced number of MPI processes on Host and an increase of OpenMP threads on the Xeon Phi.}

\par{The best performace for time per step for the 5110P was 0.5~s and 0.42~s for the 7120A card(8 MPI processes) which are slower 
than 0.3~s on the equivalent number of MPI processes on the Host. These results are encouraging as is possible to further improve 
them by using asynchronous computations and data transfers, as we used in a previous work\cite{Elena2014}. Indeed preliminary results were obtained which confirm this and will
be presented in a further communication.} 


